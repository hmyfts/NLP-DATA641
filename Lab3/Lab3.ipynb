{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aeb3357",
   "metadata": {},
   "source": [
    "## Exercise 1{-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645aa9c",
   "metadata": {},
   "source": [
    "**Importing Base Modules and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21161ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d856548",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_twt = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1d536",
   "metadata": {},
   "source": [
    "## Exercise 2{-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d20e70",
   "metadata": {},
   "source": [
    "**Extracting and Normalizing Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec292ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:\n",
      " #coronavirus #virus Reasons to buy back stocks today 1500 new cases 60 deaths 2700 recovered #markets #portfolio\n",
      "\n",
      "Tweet without hashtag:\n",
      " coronavirus virus Reasons to buy back stocks today 1500 new cases 60 deaths 2700 recovered markets portfolio\n"
     ]
    }
   ],
   "source": [
    "## Removing Hashtags\n",
    "\n",
    "dt_twt['No Hash'] = [re.sub('#', '', line) for line in dt_twt['Tweet']] ## Removing pound sign\n",
    "\n",
    "## Example: \n",
    "print('Original Tweet:\\n', dt_twt['Tweet'].iloc[100])\n",
    "print('\\nTweet without hashtag:\\n', dt_twt['No Hash'].iloc[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31091ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping words with less than 2 characters\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "twt_longs = []\n",
    "\n",
    "for tweet in dt_twt['No Hash']:\n",
    "    twt_words = word_tokenize(tweet)\n",
    "    twt_n = ' '.join([word for word in twt_words if re.match('\\w{2,}|\\d+',word)])\n",
    "    twt_longs.append(twt_n)\n",
    "    \n",
    "dt_twt['No Short Word'] = twt_longs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ab5dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 Rings Podcast Road To Tokyo March 10th 2020 COVID-19 and the Possibility of an Olympic Cancellation 24thminute KevLaramee coronavirus COVID19 Tokyo2020 IOC OlympicPodcast'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_twt['No Short Word'].iloc[326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefdfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatizing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tw_temp=[]\n",
    "\n",
    "for tweet in dt_twt['No Short Word']:\n",
    "    twt_lm = [lemmatizer.lemmatize(word) for word in word_tokenize(tweet)]\n",
    "    tw_temp.append(' '.join(twt_lm))\n",
    "\n",
    "dt_twt['Lemmatized'] = tw_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c818ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'State of emergency declared in New York a number of coronavirus patient climb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dt_twt['Lemmatized'].iloc[444])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56fff4",
   "metadata": {},
   "source": [
    "## Exercise 3{-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e84560",
   "metadata": {},
   "source": [
    "**Instantiating Vectorizer (tf-idf, PCA, Sparse to Dense)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fab4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiating tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase = True, stop_words = 'english', ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51b41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiating PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "## Setting the range of components to try\n",
    "ncomps = [5, 10, 20, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7049279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiating Dense Transformer\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class SparseToDense(TransformerMixin):\n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        return X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235814a",
   "metadata": {},
   "source": [
    "## Exercise 4{-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb75f5",
   "metadata": {},
   "source": [
    "**Setting up ML Process (Transformation, Pipeline, SVM hyperparams, CV, etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c568e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up data set\n",
    "\n",
    "X = dt_twt['Lemmatized']\n",
    "y = dt_twt['Is_Unreliable']\n",
    "\n",
    "X_count = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6956a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vectorize', tfidf),\n",
    "    ('densify', SparseToDense()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('dim_red', pca),\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54863961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparameters\n",
    "\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid'] ## kernels \n",
    "\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# parameters' grid\n",
    "params = {\n",
    "    'dim_red__n_components': ncomps,\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706f9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Cross Validation scheme for inner and outer loops\n",
    "\n",
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Setting GridSearch for inner loop\n",
    "\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "\n",
    "# Nested CV scores\n",
    "\n",
    "scores = cross_validate(grid_SVC, X = X, y = y, cv = outer_cv, \n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision','recall'],\n",
    "                        return_estimator = True)\n",
    "\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e45d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Performance: AUC, Accuracy, F1\n",
    "\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c951a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "\n",
      "Accuracy: 0.7696 \n",
      "Precision: 0.7771\n",
      "Recall: 0.76\n",
      "F1: 0.7661\n"
     ]
    }
   ],
   "source": [
    "## Average scores for models\n",
    "\n",
    "print('Average Scores:\\n')\n",
    "print('Accuracy: {} \\nPrecision: {}\\nRecall: {}\\nF1: {}'.format(\n",
    "    accuracy.mean().round(4), precision.mean().round(4), recall.mean().round(4), f1.mean().round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d982c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 0.01, 'classify__kernel': 'linear', 'dim_red__n_components': 100}\n",
      "\n",
      "\n",
      "{'classify__C': 0.01, 'classify__kernel': 'linear', 'dim_red__n_components': 100}\n",
      "\n",
      "\n",
      "{'classify__C': 0.01, 'classify__kernel': 'linear', 'dim_red__n_components': 100}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'sigmoid', 'dim_red__n_components': 50}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'linear', 'dim_red__n_components': 50}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Best Performance\n",
    "\n",
    "for score in estimators:\n",
    "    print(score.best_params_)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
